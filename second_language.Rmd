---
title: Second languages for R users?
author: Jeffrey W. Hollister
date: 2017-12-26
editor_options: 
  chunk_output_type: console
---

Sent tweet to see what twitter #rstats hashtag could rustle up on thoughts of a language to learn, in addition to , R.

# use `rtweet`

```{r, eval=FALSE}
library(rtweet)
library(dplyr)
create_token("rtweet_second_language", 
             Sys.getenv("second_language_key"),
             Sys.getenv("second_language_secret"))

# Get existing tweets
load("all_tweets.Rda")
#original_tweet <- search_tweets(q = '"#rstats twitter if you had the time and wanted to learn another language what language would you choose and why?"', include_rts = TRUE, parse = TRUE) %>%
#  arrange(created_at)
recent_mentions <- search_tweets("@jhollist", n = 500, parse = TRUE) %>%
  filter(!status_id %in% all_tweets$status_id)
all_tweets <- rbind(all_tweets, recent_mentions) %>%
  unique() %>% 
  arrange(created_at)

# This stuff is to make sure I don't grab tweets after the ~week limit on 
# Twitters search API and blow away all my saved tweets.  
# not using a csv because list columns

save(all_tweets, file = "all_tweets.Rda")
```

# Borrowing liberally from Lucy Stats!

```{r}
library("ggraph")
library("igraph")
library("ggiraph")

load("all_tweets.Rda")

orig_id <- "945766352119304193"
id <- orig_id
diff <- 1
while (diff != 0) {
id_next <- all_tweets %>%
  filter(reply_to_status_id %in% id) %>%
  pull(status_id)
id_new <- unique(c(id, id_next))
diff <- length(id_new) - length(id)
id <- id_new
}

all_replies <- all_tweets %>% 
  filter(reply_to_status_id %in% id)

from_text <- all_replies %>%
  select(reply_to_status_id) %>%
  left_join(all_replies, c("reply_to_status_id" = "status_id")) %>%
  select(screen_name, text)

tweet_0 <- all_tweets %>% filter(status_id == orig_id) %>% pull(text)

to_text <- paste0(all_replies$screen_name, ": ", all_replies$text)
to_text <- gsub("'", "`", to_text)
from_text <- paste0(from_text$screen_name, ": ", from_text$text)
from_text <- gsub("'", "`", from_text)

edges <- tibble::tibble(
  from = from_text,
  to = to_text
) %>%
  mutate(from = ifelse(
    from == "NA: NA",
    tweet_0,
    from)
  )


graph <- graph_from_data_frame(edges, directed = TRUE)
V(graph)$tooltip <- V(graph)$name

set.seed(525)
p <- ggraph(graph, layout = "nicely") + 
  geom_edge_link() + 
  geom_point_interactive(aes(x, y, color = "red", alpha = 0.05, tooltip = tooltip)) +
  theme_void() + 
  theme(legend.position = "none")
ggiraph(code = print(p),
        width_svg = 10,
        zoom_max = 4)

library("tidytext")

#this will drop links & symbols
drop_pattern <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https|ht"
unnest_pattern <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

all_replies %>% 
  mutate(text = stringr::str_replace_all(text, drop_pattern, "")) %>%
  select(text) %>%
  unnest_tokens(word, 
                text, 
                token = "regex", 
                pattern = unnest_pattern) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

# Count languages

```{r}
library(dplyr)
library(stringr)
load("all_tweets.Rda")

orig_id <- "945766352119304193"
id <- orig_id
diff <- 1
while (diff != 0) {
id_next <- all_tweets %>%
  filter(reply_to_status_id %in% id) %>%
  pull(status_id)
id_new <- unique(c(id, id_next))
diff <- length(id_new) - length(id)
id <- id_new
}

all_replies <- all_tweets %>% 
  filter(reply_to_status_id %in% id)

languages <- c("c\\+\\+", "c/c\\+\\+", "c\\+\\+17", "c\\+\\+20", "javascript",
               "python","rust","go","golang","ruby","sql","julia", "tensorflow", 
               "swift", "spark", "scratch","sas","rails","perl", "keras", 
               "julialang", "js", "java", "julia's", "html", "hive", "haskell", 
               "golang", "fortran", "ethereum", "d3", "css", "c#", "sml", 
               "assembly")
languages_b<- stringr::str_c("\\b",languages,"|\\b",languages,"\\b|",languages,"\\b")
languages_simp <- c("c", "c", "c", "c", "javascript",
               "python","rust","go","go","ruby","sql","julia", "tensorflow", 
               "swift", "spark", "scratch","sas","rails","perl", "keras", 
               "julia", "javascript", "java", "julia", "html", "hive", "haskell", 
               "go", "fortran", "ethereum", "javascript", "css", "c#", "sml", 
               "assembly")

lang_df <- data_frame(languages,languages_b, languages_simp)

all_replies<-all_replies %>% 
  mutate(clean_text = str_replace_all(str_to_lower(text),"^#|\\s#|\\.|\\?|!"," ")) 



lang_count<-data.frame(language = lang_df$languages_simp,
                       count = vector("numeric", length = length(languages)), 
                       interactions = vector("numeric", 
                                             length = length(languages)))
for(i in seq_along(languages)){
lang_count[i,2]<-all_replies$clean_text %>% 
  str_detect(lang_df$languages_b[i]) %>%
  sum() # need to do something with which() and unique.

lang_count[i,3] <- all_replies %>%
  filter(str_detect(.$clean_text, lang_df$languages_b[i])) %>%
  select(favorite_count,retweet_count) %>%
  sum()

}

# This is double counting...

lang_count %>%
  group_by(language) %>%
  summarize(total_interactions = sum(interactions)) %>%
  arrange(desc(total_interactions))



```